{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Question 1\n",
    "\n",
    "## 导入数据\n",
    "1. 导入这两列数据，并进行简单的数据预处理。\n",
    "2. 用matplotlib.pyplot画出每行两列值组成的点的分布的二维平面图。通过观察，初步上粗滤地确定聚类的簇数目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "# @Author  : Wang Huanchen <11810419@mail.sustech.edu.cn>\n",
    "# @Time    : 2021/4/30 15:35\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Question 1\n",
    "\n",
    "# Loads the data and data preprocessing.\n",
    "HW3_1 = pd.read_csv(\"HW3_1_data.csv\")\n",
    "# HW3_1.head()\n",
    "X = HW3_1.iloc[:, :].values.reshape(-1, 2)\n",
    "\n",
    "# plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用K-means聚类\n",
    "由于K-means对于非凸数据集比较难收敛，因此在通过对样本分布图的粗滤观察，来选择聚类的簇的数量范围，以及max_iter的范围，对于max_iter，较小的值可以防止迭代次数过多而更加难以收敛。\n",
    "1. 首先想到对于Kmeans，可以采用手肘法来对聚类的簇数进行确定，通过绘制不同簇数的惯性图，来找到拐点区域，确定n_clusters。\n",
    "2. 通过 1. 可以确定 n_clusters可以在 [2,4] 区间选取， 但明显可以肉眼看出簇应该在 [6,7] 这个区间， 原来手肘法并不适用与非凸数据集，因此为了尽可能的优化kmeans在非凸数据的表现，只好减小max_iter， 来让聚类更符合时间， 虽然最后的评估结果不一定是最好，但比较接近真实情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "# Standardize X.\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Use the law of the elbow.\n",
    "n_clusters = 9\n",
    "score = []\n",
    "for i in range(1, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=50)\n",
    "    kmeans.fit(X)\n",
    "    score.append(kmeans.inertia_)\n",
    "# plt.plot(score, 'bx-')\n",
    "\n",
    "# Use the K-means to clusting.\n",
    "kmeans = KMeans(n_clusters=7, max_iter=20, random_state=80).fit(X)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "# plt.show()\n",
    "\n",
    "# get the score.\n",
    "print(metrics.calinski_harabasz_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用DBSCAN聚类\n",
    "对距离阈值eps 以及 最小样本数min_samples在sklean.cluster.DBSCAN中的调参：\n",
    "1. 通过用plot展现最终结果，直接观察不同颜色的簇，来粗滤评判聚类效果：类别过少，则需要减小eps，增大min_samples，类别划分过多，增大eps,减小min_samples。 \n",
    "2. 通过上面确定的eps和min_sampls的大致范围，设定区间，然后通过循环，选出该区间中聚类效果最好的两值，并用他们来进行DBSCAN聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = HW3_1.iloc[:, :].values.reshape(-1, 2)\n",
    "\n",
    "# Find the optimal eps and min_samples for DBSCAN.\n",
    "best_eps = 0\n",
    "best_min_simples = 0\n",
    "max = -10\n",
    "for eps in np.arange(1, 2, 0.05):\n",
    "    for min_samples in range(1, 10):\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "        y_pred = dbscan.fit_predict(X)\n",
    "        if max < metrics.calinski_harabasz_score(X, y_pred):\n",
    "            max = metrics.calinski_harabasz_score(X, y_pred)\n",
    "            best_eps = eps\n",
    "            best_min_simples = min_samples\n",
    "\n",
    "# Use DBSCAN to clusting.\n",
    "dbscan = DBSCAN(eps=best_eps, min_samples=best_min_simples).fit(X)\n",
    "y_pred = dbscan.fit_predict(X)\n",
    "\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y_pred) \n",
    "# plt.show()\n",
    "\n",
    "# get the score.\n",
    "print(metrics.calinski_harabasz_score(X, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "## 数据预处理部分：\n",
    "1. 首先通过使用 **HW3_2.isnull().sum()** 发现数据中，在MINIMUM_PAYMENTS和CREDIT_LIMIT列有nan值，对此，需要进行补全，根据该两列的实际意义，都可以使用该两列的均值mean替代，因此使用 **HW3_2.fillna(HW3_2.mean(),inplace=True)**。\n",
    "2. 通过对每列数据的用途和文档中的解释以及每列数据的范围，可以大致分为三类：比列频率类，交易数类，份额类这三大类。对这三大类本别进行区别化的处理。并根据每个列的不同范围，为他们划分出不同label出来，并添加到HW3_2这个DataFrame中。并把原来的数值部分删掉，只保留新加的每个属性的所给标签值。方便后面对X的标准化以及对后续每个簇的具体含义进行理解。\n",
    "3. 将新加的16个列与TENURE列，共17个列作为X，并对X做标准化，在数据预处理层面优化X。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data and data preprocessing\n",
    "HW3_2 = pd.read_csv('HW3_2_data.csv')\n",
    "# HW3_2.isnull().sum()\n",
    "\n",
    "HW3_2.fillna(HW3_2.mean(), inplace=True)\n",
    "\n",
    "# HW3_2.sample(20)\n",
    "\n",
    "# Give each columns' values a new label for the clusting.\n",
    "RATIO = ['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n",
    "         'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\n",
    "\n",
    "for ratio in RATIO:\n",
    "    Label = ratio + '_LABEL_RANGE'\n",
    "    HW3_2[Label] = 0\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0) & (HW3_2[ratio] <= 0.1)), Label] = 1\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.1) & (HW3_2[ratio] <= 0.2)), Label] = 2\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.2) & (HW3_2[ratio] <= 0.3)), Label] = 3\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.3) & (HW3_2[ratio] <= 0.4)), Label] = 4\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.4) & (HW3_2[ratio] <= 0.5)), Label] = 5\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.5) & (HW3_2[ratio] <= 0.6)), Label] = 6\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.6) & (HW3_2[ratio] <= 0.7)), Label] = 7\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.7) & (HW3_2[ratio] <= 0.8)), Label] = 8\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.8) & (HW3_2[ratio] <= 0.9)), Label] = 9\n",
    "    HW3_2.loc[((HW3_2[ratio] > 0.9) & (HW3_2[ratio] <= 1.0)), Label] = 10\n",
    "\n",
    "TRX = ['PURCHASES_TRX', 'CASH_ADVANCE_TRX']\n",
    "\n",
    "for trx in TRX:\n",
    "    Label = trx + '_LABEL_RANGE'\n",
    "    HW3_2[Label] = 0\n",
    "    HW3_2.loc[((HW3_2[trx] > 0) & (HW3_2[trx] <= 10)), Label] = 1\n",
    "    HW3_2.loc[((HW3_2[trx] > 10) & (HW3_2[trx] <= 20)), Label] = 2\n",
    "    HW3_2.loc[((HW3_2[trx] > 20) & (HW3_2[trx] <= 30)), Label] = 3\n",
    "    HW3_2.loc[((HW3_2[trx] > 30) & (HW3_2[trx] <= 40)), Label] = 4\n",
    "    HW3_2.loc[((HW3_2[trx] > 40) & (HW3_2[trx] <= 50)), Label] = 5\n",
    "    HW3_2.loc[((HW3_2[trx] > 50) & (HW3_2[trx] <= 60)), Label] = 6\n",
    "    HW3_2.loc[((HW3_2[trx] > 60) & (HW3_2[trx] <= 70)), Label] = 7\n",
    "    HW3_2.loc[((HW3_2[trx] > 70) & (HW3_2[trx] <= 80)), Label] = 8\n",
    "    HW3_2.loc[((HW3_2[trx] > 80) & (HW3_2[trx] <= 90)), Label] = 9\n",
    "    HW3_2.loc[((HW3_2[trx] > 90) & (HW3_2[trx] <= 100)), Label] = 10\n",
    "    HW3_2.loc[(HW3_2[trx] > 100), Label] = 11\n",
    "\n",
    "VALUE = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES',\n",
    "         'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']\n",
    "\n",
    "for value in VALUE:\n",
    "    Label = value + '_LABEL_RANGE'\n",
    "    HW3_2[Label] = 0\n",
    "    HW3_2.loc[((HW3_2[value] > 0) & (HW3_2[value] <= 500)), Label] = 1\n",
    "    HW3_2.loc[((HW3_2[value] > 500) & (HW3_2[value] <= 1000)), Label] = 2\n",
    "    HW3_2.loc[((HW3_2[value] > 1000) & (HW3_2[value] <= 3000)), Label] = 3\n",
    "    HW3_2.loc[((HW3_2[value] > 3000) & (HW3_2[value] <= 5000)), Label] = 4\n",
    "    HW3_2.loc[((HW3_2[value] > 5000) & (HW3_2[value] <= 10000)), Label] = 5\n",
    "    HW3_2.loc[((HW3_2[value] > 10000)), Label] = 6\n",
    "\n",
    "# Standardize X.\n",
    "X = HW3_2.iloc[:, 17:].values.reshape(-1, 17)\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 具体的聚类过程：\n",
    "1. 选用Kmeans对X进行聚类， 由于是高维平面，没有比较好的办法，直观的看出来大致可以聚类的簇的区间，因此先设定n_clusers最大为20，并用Kmeans常用的手肘法，用matplotlib.pyplot绘图，来看惯性这一参数的明显拐点在哪里，然后用它作为最终聚类的n_clusters值。\n",
    "2. 通过图，可以确定n_clusters在 [4,6] 这个区间，再通过评判这三点的calinski_harabasz_score来最终确定选择那个合适\n",
    "3. 完成聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the law of the elbow.\n",
    "n_clusters = 20\n",
    "score = []\n",
    "for i in range(1, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=50)\n",
    "    kmeans.fit(X)\n",
    "    score.append(kmeans.inertia_)\n",
    "\n",
    "# plt.plot(score, 'rx-')\n",
    "\n",
    "max = -1\n",
    "n_clusters = 4\n",
    "for i in range(4, 7):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=50)\n",
    "    kmeans.fit(X)\n",
    "    y_pred = kmeans.fit_predict(X)\n",
    "    if max < metrics.calinski_harabasz_score(X, y_pred):\n",
    "        max = metrics.calinski_harabasz_score(X, y_pred)\n",
    "        n_clusters = i\n",
    "\n",
    "# print(n_clusters)\n",
    "\n",
    "# Use K-means to clusting.\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=50)\n",
    "kmeans.fit(X)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "print(metrics.calinski_harabasz_score(X, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
